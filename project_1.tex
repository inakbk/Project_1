\documentclass[11pt,a4wide]{article}
\usepackage{verbatim}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{a4wide}
\usepackage{color}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[dvips]{epsfig}
\usepackage[T1]{fontenc}
\usepackage{cite} % [2,3,4] --> [2--4]
\usepackage{shadow}
\usepackage{hyperref}

\usepackage{caption}
\usepackage{subcaption}

\setcounter{tocdepth}{2}

\lstset{language=c++}
\lstset{alsolanguage=[90]Fortran}
\lstset{basicstyle=\small}
\lstset{backgroundcolor=\color{white}}
\lstset{frame=single}
\lstset{stringstyle=\ttfamily}
\lstset{keywordstyle=\color{red}\bfseries}
\lstset{commentstyle=\itshape\color{blue}}
\lstset{showspaces=false}
\lstset{showstringspaces=false}
\lstset{showtabs=false}
\lstset{breaklines}

\title{Project 1, FYS-3150}
\author{Ina K. B. Kullmann}
\date{\today}
\begin{document}

\maketitle

\tableofcontents

{\scriptsize \noindent All source codes can be found at: \texttt{https://github.com/inakbk/Project\_1.git} in the folders \texttt{exercise\_b} and \texttt{exercise\_d}. }


\begin{abstract}
The goal for this project is to solve the general one-dimentional Poisson equation with two different numerical methods and compare with the exact analythical solution. For each numerical method the relative error, execution time and number of floating point operations is calculated.

The two numerical methods used to solve the equations is forward/backward substitution and LU-decomposition. Both methods are using linear algebra to turn the problem into a set of many linear equations which can be represented by matrixes. 

We will see that the execution time and relative error varies for the two methods and that increasing n gives smaller error up to a point. 
\end{abstract}


\newpage
\section{Introduction: Motivation and Purpose}

Many important differential equations in the Sciences can be written as linear second-order differential equations \footnote{Very much of the text in this section and some of the other sections is copy or moderated text from the exercise text. I wrote this report to assume that the reader did not have the exercise text --> it then felt naturally to reuse some of the text when the .tex file was given. OK, not OK?}
\[
\frac{d^2y}{dx^2}+k^2(x)y = f(x),
\]
where $f$ is normally called the inhomogeneous term and $k^2$ is a real function. It is therefore of special interest to be able to solve these kinds of equations. 

A classical equation from electromagnetism is Poisson's equation. The electrostatic potential $\Phi$ is generated by a localized charge distribution $\rho ({\bf r})$.   In three dimensions it reads
\[
\nabla^2 \Phi = -4\pi \rho ({\bf r}).
\]
This can be simplified with a spherically symmetric $\Phi$ and $\rho ({\bf r})$ to:
\[
\frac{1}{r^2}\frac{d}{dr}\left(r^2\frac{d\Phi}{dr}\right) = -4\pi \rho(r),
\]
which is a simple one-dimensional equation. Simplifying further via a substitution $\Phi(r)= \phi(r)/r$ the equation reads:
\[
\frac{d^2\phi}{dr^2}= -4\pi r\rho(r).
\]
We rewrite this equation again by letting $\phi\rightarrow u$ and 
$r\rightarrow x$. Then general one-dimensional \textbf{Poisson equation} reads:
\begin{equation}
-u''(x) = f(x). 
\label{eq:Poisson}
\end{equation}
where the inhomogeneous term $f$ (or source term) is given by the charge distribution $\rho$  multiplied by $r$ and the constant $-4\pi$.

In this project the general equation \ref{eq:Poisson} is the equation of interest that will be solved numerically. The purpose of this is to shed light on how to best solve simple linear second-order differential equations in Physics numerically by using forward/backward substitution and LU-decomposition. 

\section{Theory: Rewriting the equation into a linear algebra problem}
To solve equation \ref{eq:Poisson} we will use Dirichlet boundary conditions and rewrite the equation as a set of linear equations.

In specific we will solve:
\[
-u''(x) = f(x), \hspace{0.5cm} x\in(0,1), \hspace{0.5cm} u(0) = u(1) = 0.
\]
where we assume that the the source term is given by $f(x) = 100e^{-10x}$.  Then the above differential equation
has a closed-form analytical solution given by:
\begin{equation}
u(x) = 1-(1-e^{-10})x-e^{-10x}
\label{eq:analytical_sol}
\end{equation}

For the numerical methods we define the discretized approximation  to $u$ as $v_i$  with grid points $x_i=ih$ in the interval from $x_0=0$ to $x_{n+1}=1$.
The step length or spacing is defined as $h=1/(n+1)$. The boundary conditions gives $v_0 = v_{n+1} = 0$.
We  approximate the second derivative of $u$ with 
\begin{equation}
-\frac{v_{i+1}+v_{i-1}-2v_i}{h^2} = f_i  \hspace{0.5cm} \mathrm{for} \hspace{0.1cm} i=1,\dots, n,
\label{eq:sec_der}
\end{equation}
where $f_i=f(x_i)$.

We can rewrite this equation as a linear set of equations of the form 
\begin{equation}
   {\bf A}{\bf v} = \tilde{{\bf b}}
   \label{eq:linear_eq}
\end{equation}
by rewriting equation \ref{eq:sec_der} as
\[  
   -v_{i+1}-v_{i-1}+2v_i = h^2f_i
\]
so that ${\bf A}$ is an $n\times n$  tridiagonal matrix which we write as 
\begin{equation}
    {\bf A} = \left(\begin{array}{cccccc}
                           2& -1& 0 &\dots   & \dots &0 \\
                           -1 & 2 & -1 &0 &\dots &\dots \\
                           0&-1 &2 & -1 & 0 & \dots \\
                           & \dots   & \dots &\dots   &\dots & \dots \\
                           0&\dots   &  &-1 &2& -1 \\
                           0&\dots    &  & 0  &-1 & 2 \\
                      \end{array} \right)
\end{equation}
and the left hand side is given by $\tilde{b}_i=h^2f_i$. The total set of matrixes must then be given by:

\begin{equation}
{\bf A}{\bf v} = \left(\begin{array}{cccccc}
                           2& -1& 0 &\dots   & \dots &0 \\
                           -1 & 2 & -1 &0 &\dots &\dots \\
                           0&-1 &2 & -1 & 0 & \dots \\
                           & \dots   & \dots &\dots   &\dots & \dots \\
                           0&\dots   &  &-1 &2& -1 \\
                           0&\dots    &  & 0  &-1 & 2 \\
                      \end{array} \right)
                      \left(\begin{array}{c}
                           v_1 \\
                           v_{2}\\
                           v_{3}\\
                           \dots \\
                           v_{n-1}\\
                           v_n\\
                      \end{array} \right) = 
                       \left(\begin{array}{c}
                           h^2f_1 \\
                           h^2f_{2}\\
                           h^2f_{3}\\
                           \dots \\
                           h^2f_{n-1}\\
                           h^2f_n\\
                      \end{array} \right) = 
                      \left(\begin{array}{c}
                           \tilde{b}_1 \\
                           \tilde{b}_2\\
                           \tilde{b}_3\\
                           \dots \\
                           \tilde{b}_{n-1}\\
                           \tilde{b}_n\\
                      \end{array} \right) =
                      \tilde{{\bf b}}
\end{equation}

\subsection{Forward and Backward substitution}
We can rewrite our matrix ${\bf A}$ further in terms of one-dimensional vectors $a,b,c$  of length $1:n$:
\begin{equation}
    {\bf A} = \left(\begin{array}{cccccc}
                           b_1& c_1 & 0 &\dots   & \dots &\dots \\
                           a_2 & b_2 & c_2 &\dots &\dots &\dots \\
                           & a_3 & b_3 & c_3 & \dots & \dots \\
                           & \dots   & \dots &\dots   &\dots & \dots \\
                           &   &  &a_{n-2}  &b_{n-1}& c_{n-1} \\
                           &    &  &   &a_n & b_n \\
                      \end{array} \right)\left(\begin{array}{c}
                           v_1\\
                           v_2\\
                           \dots \\
                          \dots  \\
                          \dots \\
                           v_n\\
                      \end{array} \right)
  =\left(\begin{array}{c}
                           \tilde{b}_1\\
                           \tilde{b}_2\\
                           \dots \\
                           \dots \\
                          \dots \\
                           \tilde{b}_n\\
                      \end{array} \right).
\end{equation}

A tridiagonal matrix is a special form of banded matrix where all the elements are zero except for those on and immediately above and below the leading diagonal. The above tridiagonal system (Equation \ref{eq:linear_eq}) can be written as
\begin{equation}
  a_iv_{i-1}+b_iv_i+c_iv_{i+1} = \tilde{b}_i, \hspace{0.5cm} \mathrm{for} \hspace{0.1cm} i=1,2,\dots,n
\end{equation}
The algorithm for solving this set of equations is rather simple and requires two steps only, a decomposition and forward substitution and a backward substitution. 

\subsection{LU Decomposition}
The LU decomposition method means that we can decompose the matrix ${\bf A}$ as the product of two matrices ${\bf L}$ and ${\bf U}$:
\[
{\bf A} = {\bf L}{\bf U}
\]
 The matrix ${\bf L}$ has elements only below the diagonal (and thereby the naming lower) and a matrix ${\bf U}$ contains both the diagonal and matrix elements above the diagonal (leading to the labelling upper). The matrix ${\bf A}$ has an LU factorization if the determinant is different from zero. The LU factorization is unique if A is non-singular.
 
 This factorisation can be used to solve equation \ref{eq:linear_eq} now written as
\[
 {\bf A}{\bf v} = {\bf L}{\bf U}{\bf v} = {\bf \tilde{b}}
\]
exploiting the fact that
\[ 
{\bf U}{\bf v} = {\bf L}^{-1}{\bf \tilde{b}} = {\bf y} \Rightarrow {\bf L}{\bf y} = {\bf \tilde{b}}
\]
to find ${\bf y}$. Then further use the now known ${\bf y}$ to find ${\bf v}$ by solving the much simpler equation:
\[
{\bf U}{\bf v} = {\bf y}
\]


\section{Algorithm}
All source codes can be found at: \texttt{https://github.com/inakbk/Project\_1.git} in the folders \texttt{exercise\_b} and \texttt{exercise\_d}. 

The algorithms are structured into two \texttt{c++} programs and one \texttt{python} program. Each of the \texttt{c++} programs solves the equations for an nxn matrix, one with the algorithm for forward/backward substitution and the other with the algorithm for LU decomposition. Both algorithms are described below. After solving the equations the result vector $v$ is written to a \texttt{.txt} file along with the $x$ value and execution time. The value of n is given on the command line so that the program is executed one time for each n. 

The python program compiles and runs the \texttt{c++} program\footnote{The aim was to only have one \texttt{c++} progam, but when I tried running the \texttt{c++} code with the LU decomposition from the python script i got a compilation error. The same compilation error came when running from the terminal, but not from QT creator. So with little time to finish the project i fixed the problem by running the LU decomposition from QT Creator before running the python script. } so that the \texttt{.txt} files are given, loads and plots the data along with the analytical solution and computes and plots the relative error. 


\subsection{Forward and Backward substitution}
The purpose of the forward and backward substitution is to make row operations on both left/right hand side of the equation so that at the left hand side eventually is left with the identity matrix times the unknown vector, ${\bf A} \rightarrow {\bf A}^* \rightarrow {\bf I}$ and ${\bf \tilde{b}} \rightarrow {\bf \tilde{b}}^*$ so the solution is then the altered left hand side:
\[
{\bf I}{\bf v} = {\bf v} = {\bf \tilde{b}}^*
\]
where * is meaning that the matrix in the algorithm now obtains a new value.

In our case we have a very special tridiagonal matrix ${\bf A}$, so we only need two operations to get the identity matrix. 

We start by the forward substitution which removes the elements below the diagonal from the second comumn and down to the n-th row. The algorithm is as follows:
\begin{align*}
b_i^* &= b_i - \frac{a_i c_{i-1}}{b_{i-1}}\\
\tilde{b}_i^* &= \tilde{b}_i - \frac{a_i}{b_{i-1}}\tilde{b}_{i-1}, \hspace{0.5cm} \mathrm{for} \hspace{0.1cm} i=2,\dots,n
\end{align*}

We can also rewrite $a_i = a = -1$ and $c_i = c = -1$ for all $i$ to reduce the number of floating point operations:
\begin{align*}
b_i^* &= b_i - \frac{1}{b_{i-1}}\\
\tilde{b}_i^* &= \tilde{b}_i + \frac{\tilde{b}_{i-1}}{b_{i-1}}
\end{align*}

The backward substitution then removes all the elements above the diagonal from the next bottom row (n-1) of the matrix and up to row number one.
\begin{align*}
\tilde{b}_{i-1}^* &= \tilde{b}_{i-1} - \frac{\tilde{b}_{i}}{b_{i}}, \hspace{0.5cm} \mathrm{for} \hspace{0.1cm} i=n,\dots,2
\end{align*}
where $c= -1$ as above. This iteration does not affect the $b_i$ since elements under the diagonal, $a_i^*$, is zero after the forward substitution. 

Finally we normalize the left hand side to one to obtain the identity matrix, meaning that the right hand side is:
\[
\tilde{b}_i^{**} = \frac{\tilde{b}_i^*}{b_i}
\]
so that the solution of the set of equations is $v_i = \tilde{b}_i^{**}$. 

Then the precice number of floating point operations needed to solve this set of equations is seven times the number of rows the algorithm iterates over which is n-1. 

\subsection{LU Decomposition}
To solve the equations by LU decomposition we use the Armadillo library functions \texttt{lu(L,U,A)} and \texttt{solve(A,B)} with the simple code:
\begin{lstlisting}
    lu(L,U,A);
    vec y = solve(L,b_thilde);
    vec v = solve(U,y);
\end{lstlisting}


%Find also the precise number of floating point 
%operations needed to solve the above equations. ?!?

%How many floating point operations does the LU decomposition use to solve the set of linear equations?


\subsection{Relative error}
The relative error in the data set in relation to the analytical solution can be computed by 
\[
   \epsilon_i=log_{10}\left(\left|\frac{v_i-u_i}
                 {u_i}\right|\right),
\]
where $\epsilon_i$ is the relative error for each corresponding value of $x_i$ where $i=1,\dots, n$. 

It can be shown that the relative error is constant over the whole data set (for one $n$). We will therefor choose the relative error for each $n$, $\epsilon$ to be the mean of the relative error for the whole dataset, $\epsilon_i$
\[
\epsilon = \frac{\epsilon_1 + \epsilon_2 + \dots + \epsilon_n}{n}
\]
We will let n be an element of $N = [10^1, 10^2, \dots, 10^5]$ plot the relative error for all the different $n$-values as a function of $log_{10}(h)$ 

\subsection{Calculating execution time}
To compute the execution time in \texttt{c++} the following statements where used:
\begin{lstlisting}[title={Time in C++}]
using namespace std;
...
#include "time.h"   //  you have to include the time.h header
int main()
{
    // declarations of variables 
    ...
    clock_t start, finish;  //  declare start and final time
    start = clock();
    // your code is here, do something and then get final time
    finish = clock();
    ( (finish - start)/CLOCKS_PER_SEC );
...
\end{lstlisting}

The timing only enclosed the code which did the calculations, not the code which generated the arrays/matrixes neded to do the calculations or the code which wrote the data to file. 

\section{Results}

\begin{figure*}[ht!]
    \centering
     \caption{Plots of the numerical and analytical solutions for $n=10, 100$.}
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=2.5in]{exercise_b/linear_eq_solution_plot_all_num_n10.eps}
        %\caption{Lorem ipsum}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=2.5in]{exercise_b/linear_eq_solution_plot_all_num_n100.eps}
        %\caption{Lorem ipsum, lorem ipsum,Lorem ipsum, lorem ipsum,Lorem ipsum}
    \end{subfigure}
   \label{fig:n_small}
\end{figure*}

\begin{figure*}[ht!]
    \centering
    \caption{Plots of the numerical and analytical solutions for $n=10^3, 10^4$.}
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=2.5in]{exercise_b/linear_eq_solution_plot_all_num_n1000.eps}
        %\caption{Lorem ipsum}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=2.5in]{exercise_b/linear_eq_solution_plot_all_num_n10000.eps}
        %\caption{Lorem ipsum, lorem ipsum,Lorem ipsum, lorem ipsum,Lorem ipsum}
    \end{subfigure}
    \label{fig:n_big}
\end{figure*}

\begin{figure}[ht!]
\caption{Plots of the relative error and execution time.}
\centering
\includegraphics[width=0.8\textwidth]{exercise_b/linear_eq_time_plot_all_num_N10000.eps}
\label{fig:error}
\end{figure}

In figures \ref{fig:n_small} and \ref{fig:n_big} we see the two numerical methods plotted together with the analytical solution for n in $N = [10^1, 10^2, 10^3, 10^4]$. The backward/forward substitution method (blue) is clearly closest to the analytical solution (red) for low n. For bigger n the LU decomposition method lookes just as good as the substitution method with 'by eye' estimates. In figure \ref{fig:error} we see that the relative error for the two methods is reduced for bigger n (smaller h), but the error is clearly smallest for the substitution method for all n. For $n=10^4$ the error is about a factor $\approx10^3$ smaller for the substitution method than the LU decomposition method. We also see that the execution time is smaller for the substitution method, as much as by a factor $\approx10^6$ for $n=10^4$!

The LU decomposition method did not work for $n>10^4$, then the program crashed, but the substitution method did not crash. For fun the last plot of this is in figure \ref{fig:tull} we see the errors and execution time for $N = [10^1, \dots, 10^8]$ for the substitution method and $N = [10^1, 10^2, 10^3, 10^4]$ for the LU decomposition method. We see that the relative error gets smaller and smaller untill $n=10^6$ where it rises to the roof and stays there for higher n. But we can also see that the execution time does not rise rapidly at $n=10^6$, but increases evenly. The execution time for $n=10^8$ for the substitution method is still less than the execution time for $n=10^4$ for the LU decomposition method. 

\begin{figure}[htp]
\caption{Plots of the relative error and execution time for ridiculous large n.}
\centering
\includegraphics[width=0.8\textwidth]{exercise_b/linear_eq_time_plot_all_num_N100000000.eps}
\label{fig:tull}
\end{figure}

%Include your results either in figure form or in a table. Remember to label your results. All tables and figures should have relevant captions and labels on the axes.
\section{Discussion and experiences}

We have seen that the forward/backward substitution method gives both less relative error and smaller execution time than the LU decomposition method from the Armadillo library. This is as expected because the substitution method is exploiting the very special tridiagonal form of the matrix ${\bf A}$. The LU decomposition algorithm must be much more general to solve all kinds of sets of equations. This generality is nice because one would not have to do so much analytical work befor solving the equations, but is lead to more round-off/owerflow errors which gives loss of precition and also a longer execution time. The factor of $\approx10^6$ longer execution time for the LU decomposition method for $n=10^4$ is a lot more than I expected! 

It was interesting to see that the LU decomposition method did not work for $n>10^4$, I did not expect that. But I did expect the relative error to increase after a point for very large n. It was impressive to see that the execution time for large n did not increase drasticaly as the relative error did. As mentioned the LU decomposition method cannot be uset for large matrixes, but I would aviod using it anyway for matrixes where other possible smart methods can be used, unless loss of precition and execution time is not important. Another point is that iIt is always smart to do some analytical work to check if the results are making sense, as for the substitution method in this project. 

Unfortunately I did not have time to try to implement the \texttt{new} and \texttt{delete} commands for dynamically memory allocation, I am using c++ for the first time. But I did construct the program such that the vectors and matrixes scaled up/down for the different n. 

But it was nice to repeat some linear algebra and imlement the methods to solve huge systems of differential exuations in Physics. And of course, really smar and fun to start programing \texttt{c++}!

\end{document}
